The actual computation area is not very big

GPUS are throughput-oriented (vs latency-oriented)

Le unità di controllo delle GPU sono molto semplici, quindi sono piu' piccole, quindi ce ne possono essere molte

- non c'e' branch prediction ne' esecuzione fuori ordine

32-thread blocks (called warps) share one controlling unit (execute the same assembly instruction)

Each ALU has a higher latency, but there are many of them, so 

The many cores allow us to mask the latency in memory accesses

GPUs use high bandwith memory (way higher than CPU and DRAM) (needs to be higher because so many cores are trying to access it)
(CPU works on DRAM, GPU has its own memory)

The main difference between CPUs and GPUs is that CPUs are oriented towards applciations that require low latency, while GPU [l'inverso].


The traditional way to combine CPUs and GPUs

![[CPU-GPU-architecture.png|center|500]]

CUDA moves data to the GPU, then takes the results from the GPU. This moving of data has some performance demands.

---
Il problema di CUDA e' che funziona solo su GPU nvidia (anche se ormai esistono tool per compilare per GPU AMD)

- **HIP** is AMD's equivalent of CUDA 
- most calls are the same, with only the first 4 characters changing (CUDA-command ⟶ HIP-command)

other options are OPENCL and OPENACC (portable both to GPs and CPU/GPUs)

---
CUDA enables a general-purpose programming model on NVIDIA GPUs
- initially created for 3d graphics, then adapted to the general public

It enables explicit GPU memory management.

The GPU is viewed as a **compute device** that:
- is a *co-processor* to the CPU
- has its own DRAM ("global memory")
- runs many threads in parallel

>[!summary] CUDA program structure
> 1) **Allocate** GPU memory
> 2) **Transfer data** from host to GPU memory
> 3) Run CUDA **kernel** (functions executed on the GPU)
> 4) (when the kernel is done) **Copy results** from GPU memory to host memory


